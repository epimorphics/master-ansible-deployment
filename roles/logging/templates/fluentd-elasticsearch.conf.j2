# fluentd/conf/fluent.conf
#
# source 
<source>
  @type forward
  bind 0.0.0.0
  port {{ fluentd_container.port.logging }}
</source>

<source>
  @type syslog
  bind 0.0.0.0
  port {{ fluentd_container.port.syslog }}
  message_length_limit 4096
# message_length_limit 16000
  tag syslog
  source_hostname_key server
</source>

# expose metrics in prometheus format
<source>
  @type prometheus
  bind 0.0.0.0
  port {{ fluentd_container.port.metrics }}
  metrics_path /metrics
</source>

<source>
  @type prometheus_output_monitor
  interval 10
  <labels>
    hostname ${hostname}
  </labels>
</source>

<system>
# log_level debug
# log_level warn
  log_level error
</system>

# filter out blanks in docker logs
<filter **>
  @id blanks
  @type grep
  <exclude>
    key message
    pattern /^\s*$/
  </exclude>
</filter>

# set s3 low by default
<filter **>
  @id init_s3
  @type record_transformer
  <record>
    s3 'false'
  </record>
</filter>

# Normalise fluentd 
<filter fluent.**>
  @id fluentd
  @type record_transformer
  <record>
    host '{{ ansible_hostname }}'
    ident 'fluentd'
  </record>
</filter>

# Transform tag
<match syslog.**>
  @id rewrite_container_name
  @type rewrite_tag_filter
  <rule>
    key ident
    pattern ^(.+)$
    tag $1.${tag}
  </rule>
</match>

# drop influxdb
<match influxdb.**>
  @id drop_influxdb
  @type null
</match>

# Archive proxy and ssh
<filter apache.** sshd.**>
  @id enable_s3
  @type record_transformer
  <record>
    s3 'true'
  </record>
</filter>

{{ fluentd.custom }}

# Add level
<filter amazon-ssm-agent.syslog.** dnf.syslog.** dhclient.syslog.** logger.syslog.** NetworkManager.syslog.** nginx.syslog.daemon.info postfix*.syslog.** rsyslogd.syslog.** sshd.syslog.** sudo.syslog.** systemd*.syslog.**>
  @id syslog_level
  @type record_transformer
  <record>
    log_level ${tag_parts[3]}
  </record>
</filter>

# Transform JSON TS tag
<match alertmanager.syslog.daemon.* dms.syslog.daemon.* fuseki.syslog.daemon.* prometheus.syslog.daemon.* sensu-am.syslog.daemon.*>
  @id rewrite_json_ts_tag
  @type rewrite_tag_filter
  @log_level warn
  <rule>
    key ident
    pattern ^(.+)$
    tag json.ts.${tag_parts[3]}.$1
  </rule>
</match>

# Transform JSON tag
<match elasticsearch.syslog.daemon.info grafana.syslog.daemon.* kibana.syslog.daemon.* nginx.syslog.daemon.info sensu.syslog.daemon.* sensu-agent.syslog.daemon.*>
  @id rewrite_json_tag
  @type rewrite_tag_filter
  @log_level warn
  <rule>
    key ident
    pattern ^(.+)$
    tag json.${tag_parts[3]}.$1
  </rule>
</match>

# Nginx err parser
<filter nginx.syslog.daemon.err>
  @id nginx_err
  @type parser
  @log_level warn
  key_name message
  remove_key_name_field true
  inject_key_prefix log_
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /(?<time>[0-9: /]*)\[(?<level>[^\]]*)\] (?<pid>[0-9]+)#(?<tid>[0-9]+): (?<message>.*)$/
    time_format %Y/%m/%d %H:%M:%S
  </parse>
</filter>

# Transform Proxy tag
<match proxy.syslog.daemon.*>
  @id rewrite_proxy_tag
  @type rewrite_tag_filter
  @log_level warn
  <rule>
    key ident
    pattern ^(.+)$
    tag apache.${tag_parts[3]}.$1
  </rule>
</match>


# Syslog info parser
<filter logger.syslog.**>
  @id syslog_rfc5424
  @type parser
  @log_level warn
  key_name message
  remove_key_name_field true
  inject_key_prefix log_
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^(?<rfc5424_version>[^ ]*) (?<msgtime>[^ ]*) (?<container_id>[^ ]*) (?<subsystem>[^ ]*) (?<procid>[^ ]*) (?<msgid>[^ ]*) \[(?<container>[^ ]*)@(?<step>[^ ]*) cmd=\"(?<command>[^ ]*)\"\] (?<message>[^$]+)/
  </parse>
</filter>


# Apache info parser
<filter apache.info.**>
  @id parser_apache
  @type parser
  @log_level warn
  key_name message
  remove_key_name_field true
  inject_key_prefix log_
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /{{ apache_info_regexp}}/
  </parse>
</filter>


# Apache info parser
<filter apache.info.**>
  @id apache_nil
  @type record_transformer
  enable_ruby true
  <record>
    log_error_id ${record["log_error_id"] == '-' ? nil : record["log_error_id"]}
    log_host     ${record["log_host"]     == '-' ? nil : record["log_host"]}
    log_qos_cr   ${record["log_qos_cr"]   == '-' ? nil : record["log_qos_cr"]}
    log_qos_com  ${record["log_qos_com"]  == '-' ? nil : record["log_qos_com"]}
    log_qos_ev   ${record["log_qos_ev"]   == '-' ? nil : record["log_qos_ev"]}
    log_referrer ${record["log_referrer"] == '-' ? nil : record["log_referrer"]}
    log_size     ${record["log_size"]     == '-' ? nil : record["log_size"]}
    log_user     ${record["log_user"]     == '-' ? nil : record["log_user"]}
    log_request_time ${(((record["log_duration"].to_f)/1000).ceil.to_f/1000)}
    log_duration ${((record["log_duration"].to_f)/1000).ceil}
  </record>
</filter>


<filter apache.err.**>
  @id parser_apache_error
  @type parser
  @log_level warn
  key_name message
  reserve_data true
  remove_key_name_field true
  inject_key_prefix log_
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^(\[(?<time>[^]]+)\] )?(\[(?<module>[^:]+):(?<level>[^ ]+)\] )?(\[pid (?<pid>\d+):tid (?<tid>\d+)\] )?(\[client (?<client>[^:]+):\d+\] )?(mod_qos\((?<qos>\d{3})\): )?(?<message>.+)?/
    time_format %a %b %d %H:%M:%S.%N %Y
  </parse>
</filter>


<filter apache.err.**>
  @id qos_scoreboard
  @type parser
  key_name log_scoreboard
  reserve_data true
  remove_key_name_field true
  inject_key_prefix log_
  <parse>
    @type json
  </parse>
</filter>


## Apache error parser
# This built in parser doesn't work
# very well especially with QoS enabled.
# <filter apache.err.**>
#   @id parser_apache_error
#   @type parser
#   @log_level warn
#   key_name message
#   reserve_data true
#   remove_key_name_field false
#   inject_key_prefix log_
#   emit_invalid_record_to_error false
#   <parse>
#     @type apache_error
#   </parse>
# </filter>

# Ignore apache server-status
<filter apache.info.proxy>
  @id apache-server-status
  @type grep
  <exclude>
    key log_path
    pattern /server-status/
  </exclude>
</filter>


# Elda
<filter elda.syslog.daemon.*>
  @id parser_elda
  @log_level warn
  @type parser
  key_name message
  remove_key_name_field true
  inject_key_prefix log_
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^(?<time>[0-9 ,-:]*) (?<level>[^ ]*) \[(?<trace>[^ ]*)\] \((?<logger>[^ ]*)\) - (?<request>Request [^ ]*)?((?<request>Response [^ ]*) (?<status>[^ ]*) (?<time_taken>.*))?(?<text>.+)?/
    time_format %Y-%m-%d %H:%M:%S,%N
  </parse>
</filter>


# Fuseki
<filter fuseki.syslog.daemon.*>
  @id parser_fuseki
  @log_level warn
  @type parser
  key_name message
  remove_key_name_field true
  inject_key_prefix log_
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^\[(?<time>[^\]]*)\] Fuseki\s+(?<level>[^ ]+)\s+\[[^ ]+\]\s+((?<method>(GET|POST)) (?<request>.+))?(Query = (?<query>.+))?((?<status>[^\(]*)\((?<elapsed>.+)\))?(?<text>.+)?/
    time_format %Y-%m-%d %H:%M:%S
  </parse>
</filter>


# Containerd Exporter Parser
<filter es_exporter.syslog.daemon.info>
  @id parser_containerd
  @type parser
  @log_level warn
  key_name message
  remove_key_name_field true
  inject_key_prefix log_
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^time=(?<timespamp>[^ ]*)\s+level=(?<level>[^ ]*)\s+msg=(?<msg>[\S" ]*)\s+id=(?<containerd_id>[\S]*)/
  </parse>
</filter>

# ES Exporter Parser
<filter es_exporter.syslog.daemon.info>
  @id parser_es_exporter
  @log_level warn
  @type parser
  key_name message
  remove_key_name_field true
  inject_key_prefix log_
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^level=(?<level>[^ ]*)\s+ts=(?<timestamp>[^ ]*)\s+caller=(?<caller>[^ ]*)\s+msg=(?<msg>[\S "]*)/
  </parse>
</filter>

# JSON parser
<filter json.**>
  @id json_level
  @type record_transformer
  @log_level warn
# @log_level debug
  <record>
    level ${tag_parts[1]}
  </record>
  @type parser
  key_name message
  reserve_data true
  reserve_time true
  remove_key_name_field false
  inject_key_prefix log_
  emit_invalid_record_to_error false
  <parse>
    @type json
  </parse>
</filter>

# Grafana hacks
<filter json.info.grafana>
  @id grafana_duration
  @type record_transformer
  enable_ruby
  <record>
    log_time_taken ${record["log_duration"]}
    log_epoch ${Time.parse(record["log_t"]).to_f.round(3)}
  </record>
  renew_time_key log_epoch
  remove_keys log_duration,log_epoch
</filter>

<filter json.info.grafana>
  @id grafana_license
  @type grep
  <exclude>
    key log_logger
    pattern /licensing/
  </exclude>
</filter>

# normalise log_level
<filter ansible*.** apache.** dhclient.syslog.** es_exporter.syslog.** postfix*.syslog.** sshd.syslog.** sudo.syslog.** systemd*.syslog.**>
  @id level_upcase
  @type record_transformer
  enable_ruby true
  <record>
    log_level ${if record.has_key?("log_level"); record ["log_level"].upcase; elsif record.has_key?("level"); record ["level"].upcase; else "INFO"; end}
  </record>
</filter>


# Data API request time
<filter json.info.data-api>
  @id data_api_duration
  @type record_transformer
  enable_ruby
  <record>
    log_request_time ${record.has_key?("log_duration") ? (record["log_duration"].to_f/1000) : nil}
  </record>
</filter>


# Nginx timestamp
<filter json.info.nginx>
  @id nginx_epoch
  @type record_transformer
  <record>
    s3 'true'
  </record>
  renew_time_key log_epoch
  remove_keys log_epoch
</filter>

# Update timestamp
<filter json.ts.**>
  @id json_epoch
  @type record_transformer
  enable_ruby
  <record>
    log_epoch ${Time.parse(record['log_ts']).to_f.round(3)}
  </record>
  renew_time_key log_epoch
  remove_keys log_epoch
</filter>

# Cleanup timestamp
<match json.ts.**>
  @id cleanup_json_ts_tag
  @type rewrite_tag_filter
  @log_level warn
  <rule>
    key ident
    pattern ^(.+)$
    tag json.${tag_parts[2]}.$1
  </rule>
</match>

<filter json.**>
  @id raw_json
  @type record_transformer
{% if keep_raw_logs is defined %}
{% if keep_raw_logs %}
# Keep raw
  <record>
    raw ${record["message"]}
  </record>
{% endif %}
{% endif %}
  remove_keys message
</filter>

# copy to elastic search
<match json.** **>
  @id out_es
  @type copy
  @log_level warn
  <store>
    @type elasticsearch
    host elasticsearch
    port {{ elasticsearch_container.port }}
    scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
    ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
    ssl_version "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERSION'] || 'TLSv1_2'}"
    user "#{ENV['FLUENT_ELASTICSEARCH_USER'] || use_default}"
    password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD'] || use_default}"
    logstash_format true
    logstash_prefix fluentd
    logstash_dateformat %Y%m%d
    reconnect_on_error true
    reload_on_failure true
    reload_connections true
    include_tag_key true
    type_name access_log
    tag_key @log_name
    <buffer>
      retry_forever true
      retry_type periodic
      retry_wait 60
    </buffer>
  </store>
  <store>
    @type relabel
    @label @S3
  </store>
  <store>
    @type stdout
  </store>
</match>


<label @S3>
  <filter **>
    @type grep
    <regexp>
      key s3
      pattern /true/
    </regexp>
  </filter>

  <match **>
    @type s3
    @id out_s3
    @log_level info
#   @log_level debug
    s3_bucket "#{ENV['S3_BUCKET_NAME']}"
    s3_region "{{ region }}"
    index_format %06d
    path logs/${host}
    utc
    format json
    s3_object_key_format %{path}/%Y-%m-%d/${tag}-%H-%{index}.%{file_extension}
    <inject>
      time_key time
      tag_key tag
    </inject>
    <buffer tag,time,host>
      @type file
      path {{ fluentd.buffer }}
      timekey 3600
#     timekey 300
      timekey_wait 1m
      timekey_use_utc true # use utc
      chunk_limit_size 256m
    </buffer>
  </match>
</label>

# count number of incoming records per tag
<filter *.**>
  @id prometheus_in
  @type prometheus
  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      tag ${tag}
      hostname fluentd.${hostname}
    </labels>
  </metric>
</filter>

# count number of outgoing records per tag
<match *.**>
  @id prometheus_out
  @type copy
  <store>
    @type forward
    <server>
      name {{ ansible_nodename }}
      host localhost
      port {{ fluentd_container.port.logging }}
      weight 60
    </server>
  </store>
  <store>
    @type prometheus
    <metric>
      name fluentd_output_status_num_records_total
      type counter
      desc The total number of outgoing records
      <labels>
        tag ${tag}
        hostname fluentd.${hostname}
      </labels>
    </metric>
  </store>
</match>

